# Chapter 02

## Accessing corpora
* *corpora*: A large body of text.

### *Gutenberg corpus and usage example*:

```python
from nltk.corpus import gutenberg
emma = gutenberg.words('austen-emma.txt')
```

*Three statistics for a language:* 
1. average word length: `len(..raw(fileid))/len(..words(fildid))`
	* General feature in English
	* *Note*: "raw" gives the contents without any linguistic processing.
2. Average sentence length: `len(..word(id))/len(..sents(id))`
3. Number of times each vocab words appears in text on average: `num(words)/num(vocab)`

### Web and Chat Text

```python
from nltk.corpus import webtext
```
* Contains text from discussion forum, conversations, movie scripts, personal advertisements, wine reviews, etc.

### Brown Corpus


```python
from nltk.corpus import brown
brown.categories()
```

* First million-word electrionic corpus of English. 
* Good for studying systematic differences between genres. *stylistcs* in Linguistics.
* Example: usage of modal verbs.

```python
from nltk.corpus import brown
news_text = brown.words(categories='news') # fetch news words
fdist = nltk.FreqDist(w.lower() for w in news_text) #Convert to lower case for easier processing
modals = ['can','could','may','might','must','will']
for m in modals:
	print(m+':',fdist(m),end=" ") # Print out the count respectively.

#Next: Conditional frequency distributions
cfd = nltk.ConditionalFreqDist((genre,word) 
	for genre in brown.categories()
	for word in brown.workds(categories = genre))
genres = ['news','religion','hobbies','science_fiction','romance','humor']
modals = ['can','could','may','might','must','will']
cfd.tabulate(conditions=genres,samples=modals) #make a table
```

### Reuters Corpus
```python
from nltk.corpus import reuters
reuters.fileids()
reuters.categories()
```
* Over 10,000 news documents, many overlap since they contain the same topic.
* Fetch via categories or fileids.

### Inaugural Address Corpus 

```python
from nltk.corpus import inaugural
inaugural.fileids()
# Use of Conditional Freq Dist again
cfd = nltk.ConditionalFreqDist((target,fileid[:4]) #[:4] extracts the years
	for fileid in inaugural.fileids() # years
	for w in inaugural.words(fileid) # get the target words
	for target in ['america','citizen']) # Specify the target

cfd.plot() #Plots two dashed lines
```
### Annotated Text corpora
* A list of corpora with linguistic tags can be found [here](http://www.nltk.org/book/ch02#tab-corpora "corpora")

### Corpora in other languages 
* [Chapter 3.3](http://www.nltk.org/book/ch03.html#sec-unicode) specified the use of other corpora. (**Encoding matters!**)
* *udhr*: Universal Declaration of Human Rights in over 300 languages

```python
fileids = udhr.fileids()
raw_text = udhr.raw('some-language')
nltk.FreqDist(raw_text).plot()
```

### Text corpus structure
* Isolated: just a collection of texts.
* Categorized: Correspond to genre, source, author, language.
* Overlappint: in case of topical categories, text can be relevant to many topics(e.g. Reuters)
* Temporal: News, Addresses.
* **Basic corpus Functionalities:**
	1. fileids([categories])
	2. categories([fileids])
	3. raw(fileids|categories=[...])
	4. words(fileids|categories=[...])
	5. sents(fileids|categories=[...])
	6. abspath(fileid) (Location of the given file on disk)
	7. endoding(fileid)
	8. open(fileid)
	9. readme()
	10. root

### Loading your own corpus: Use PlainTextCorpusReader or BracktParseCorpusReader
```python
from nltk.corpus import PlaintextCorpusReader 
corpus_root = '/path/to/folder/with/texts'
wordLists = PlaintextCorpusReader(corpus_root,'.*') #pass in list of fields or a regex pattern that matches all fileids as the second arg
wordLists.fileids()
wordlists.words(fileid)


```
[Source code to BracketParseCorpusReader](http://www.nltk.org/book/ch02#corpus-root-treebank)

## Conditional Frequency Distributions
* FreqDist() would compute the number of occurrences of each item in the list. 
* **Conditional Frequency distribution** is a collection of frequency distributions under different categories/conditions, usually the category of the text.

### Conditions and Events
* *Events*: The appearance of words in a text 
* *Conditions* : the condition specified. 
* To process conditional frequency, we need to pass in a list of pairs of conditions and events. e.g `[('news','The'),('news','County')]`

### Counting words by genres
```python
from nltk.corpus import brown
# Get condition-event pairs
genre_word = [(genre, word) 
			 for genre in ['news','romance'] 
			 for word in brown.words(categories=genre)]
cfd = nltk.ConditionalFreqDist(genre_word)
cfd.conditions() # Check the conditions input
cfd['romance'].most_common(20) # Get the most frequent 20 words in romance
cfd['romance']['could'] #Get the frequency of "could" in Romance genre
```

### Plotting and Tabulating Distributions
```python
from nltk.corpus import inaugural
cfd = nltk.ConditionalFreqDist(
	(target,fileid[:4]) # Pair for every instance of a word whose lowercased form starts with America
	for fileid in inaugural.fileids
	for w in inaugural.words(fileid)
	for target in ['america','citizen']
	if w.lower().startswith(target))


```
[sample plot generated by above code](http://www.nltk.org/book/ch02#fig-inaugural2)

* Specify a condition to display with `conditions=` parameter
* Limit the samplse to display with a `samples=` parameter

```python
cfd = nltk.ConditionalFreqDist(
	(lang, len(word))
	for lang in languages
	for word in udhr.words(lang+'-Latin1')) #filenames end with -Latin1 as the encoding method 
# Display word frequency of words that has <10 characters. 
cfd.tabulate(conditions=['america'],samples=range(10),cumulative = True)

```
* Exercise

```python
from nltk.corpus import brown
days =['monday','tuesday','wednesday','thursday','friday','saturday','sunday']
cfd = nltk.ConditionalFreqDist(
	[(genre,day)
	for genre in ['news','romance']
	for w in brown.words(categories=genre)
	for day in days
	if w.lower()==days])
cfd.tabulate(samples=days)
```

### Generating Random text with Bigrams
* bigrams() function takes a list of words and builds a list of consecutive word pairs.
* Generating random model: randomly choose the next word from among the available words.

```python
def generate_model(cfdist, word, num=15):
	# word is the context.
	for i in range(num):
	    # print out the word
		print (word, end=' ') 
		# Get the word that appears most often after the context, and set it as the new context to generate new word.
		word = cfdist[word].max()
text = nltk.corpus.genesis.words('english-kjv.txt')
bigrams = nltk.bigrams(text)
cfd = nltk.ConditionalFreqDist(bigrams)
# Easy to stuck in loop. Another method will be randomly choose the next wordd from among the available words.
```

* Commonly-used method of cfd:
	1. .conditions()
	2. cfdist[condition][sample]
	3. .tabulate(samples,conditions)
	4. .plot(samples,conditions)
	5. cfdist1<cfdist2 #Test if samples in cfdist1 occur less frequently than in cfdist2.

## More python: Reusing code
### Notes omitted as I am pretty familiar with this part 

## Lexical Resources
* Lexicon: collection of words/phrases along with associated information 
* *lexical entry*: a *headword(lemma)* along with additional information such as part of speech and sense definition. 
* Two distinct words have the same spelling are called *homonyms*.

### Wordlist corpora
* Can be used for spell-checkers
* words corpus

```python
def unusual_words(text):
	text_vocab = set(w.lower() for w in text if w.isalpha())
	english_vocab = set(w.lower() for w in nltk.corpus.word.words())
	unusual = text_vocab - english_vocab
```

* stopwords corpus

```python
from nltk.corpus import stopwords
stopwords.words('english')
def content_fraction(text):
	stopwords = nltk.corpus.stopwords.words('english')
	content = [w for w in text if w.lower not in stopwords]
	return len(content)/len(text)
```
* Stopwords can be used to filter out words. 
* Example use of word-list : solving a [word puzzle](http://www.nltk.org/book/ch02#fig-target)

```python
puzzle_letters = nltk.FreqDist('egivrvonl')
obligatory = 'r'
wordlist = nltk.corpus.words.words()
[w for w in wordlist if len(w) >=6 
					 and obligatory in w
					 and nltk.FreqDist{w} <= puzzle_letters]
# Note: FreqDist comparison method permits us to check the frequency of each letter in the cadidate word is less than or equal to the frequency of the corresponding letter in the puzzle.

```
* Name corpus 
	* Contains 8,000 first names categorized by gender.

### Pronouncing dictionary
* CMU pronouncing dictionary of US English: for each word, this lexicon provides a list of phonetic codes-known as *phones*. Symbols are from [Arpabet](http://en.wikipedia.org/wiki/Arpabet)
* Useful for finding rhyming words
* Useful for finding mismatches between pronounciations and writing. 
* Contains digits to represent primary stress(1),secondary stress(2) and no stress(0)

```python
# Finding rhyming words
syllable = ['N',"IHO","K","S"]
[word for word, pron in entries if pron[-4:]==syllable]

# Finding a particular stress pattern

# Extract the stress digit
def stress(pron):
	return [char for phone in pron for char in phone if char.isdigit()] 

[w for 2, pron in entries if stress(pron)==['0','1','0','2','0']]

# Accessing the dictionary by looking up particular words
prondict = nltk.corpus.cmudict.dict()
prondict['fire']

```

### Comparative wordLists
* Swadesh: Lists of about 200 common words in serveral languages.

```python
# Access cognate words from multiple languages using the entries() method.
from nltk.corpus import swadesh
fr2en = swadesh.entries(['fr','en'])
translate = dict(fr2en)
de2en = swadesh.entries(['de','en'])
translate.update(dict(de2en))

# Compare Germanic and Romance languages
languages = ['en','de','nl','es','fr','pt','la']
for i in [139,140,141,142]:
	print (swadish.entries(languages)[i])
```

### Shoebox and toolbox lexicons
[Download here](http://www.sil.org/computing/toolbox/)
* Consists of a collection of entries.

## Wordnet
* A semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure.

### Senses and Synonyms
* *Synonyms* : words that have the same meaning 

```python
from nltk.corpus import wordnet as wn 
wn.synsets('motocar') 
```
Here it outputs ('car.n.01') The car.n.01 entity is called a *synset*, or 'synonym set', a collection of synonyous words (lemmas)

```python
# Get the synonyous words
wn.synset('car.n.01').lemma_names 
wn.synset('car.n.01').definition()
wn.synset('car.n.01').examples()

# Get all the lemmas for a given synset
wn.synset('car.n.01').lemmas()
# Look up a particular lemma
wn.synset('car.n.01.automobile')
```

* Unlike motorcar, the word *car* is ambiguous. It has five synsets .

Access all the lemmas involving the word car:

`wn.lemmas('car')`

### Wordnet Hierarchy

* Wordnet makes it easy to navigate between concepts.
* Lexical relations: hyponyms and hypernyms 
* Looking up **hyponyms** : 

```python
motorcar = wn.synset('car.n.01')
types_of_motorcar = motorcar.hyponyms()
sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())
```

* Looking up **hypernyms**:

```python
motorcar.hypernyms()
paths = motorcar.hypernym_paths() # Some words have multiple paths because they can be classified in more than one way. 
len(paths)
[synset.name() for synset in paths[0]]
[synset.name() for synset in paths[1]]
```

### More lexical relations 
* **meronyms**: from items to their components
* **holonyms**: Things items are contained in. 

```python
wn.synset('tree.n.01').part_meronyms() # Parts of a tree. eg. trunks
wn.synset('tree.n.01').substance_meronyms # Substance of a tree eg. heartwood
wn.synset('tree.n.01').member_holonyms()

wn.synset('walk.v.01').entailments() # Walking entails stepping, as the act of walking involves the act of stepping .
```

### Semantic similaritiy

* Knowing which words are semantically related is useful for indexing a collection of texts, so that a search for a general term like vehicle will match documents containing specific term like limousine.

```python
right = wn.synset('right_whale.n.01')
minke = wn.synset('minke_whale.n.01')
tortoise  = wn.synset('tortoise.n.01')
right.lowest_common_hypernyms(minke) #baleen_whale.n.01
right.lowest_common_hypernyms(tortoise) #vertebrate
```

* Quantify the generality of the hypernyms:

`wn.synset('baleen_whale.n.01').min_depth() #14`

`right.path_similarity(minke)`

* type help(wn) for more information. 










