# Chapter 02

## Accessing copora
* *corpora*: A large body of text.

### *Gutenberg corpus and usage example*:

```python
from nltk.corpus import gutenberg
emma = gutenberg.words('austen-emma.txt')
```

*Three statistics for a language:* 
1.average word length: `len(..raw(fileid))/len(..words(fildid))`
	* General feature in English
	* *Note*: "raw" gives the contents without any linguistic processing.
2.Average sentence length: `len(..word(id))/len(..sents(id))`
3.Number of times each vocab words appears in text on average: `num(words)/num(vocab)`

### Web and Chat Text

```python
from nltk.corpus import webtext
```
* Contains text from discussion forum, conversations, movie scripts, personal advertisements, wine reviews, etc.

### Brown Corpus


```python
from nltk.corpus import brown
brown.categories()
```

* First million-word electrionic corpus of English. 
* Good for studying systematic differences between genres. *stylistcs* in Linguistics.
* Example: usage of modal verbs.

```python
from nltk.corpus import brown
news_text = brown.words(categories='news') # fetch news words
fdist = nltk.FreqDist(w.lower() for w in news_text) #Convert to lower case for easier processing
modals = ['can','could','may','might','must','will']
for m in modals:
	print(m+':',fdist(m),end=" ") # Print out the count respectively.

#Next: Conditional frequency distributions
cfd = nltk.ConditionalFreqDist((genre,word) 
	for genre in brown.categories()
	for word in brown.workds(categories = genre))
genres = ['news','religion','hobbies','science_fiction','romance','humor']
modals = ['can','could','may','might','must','will']
cfd.tabulate(conditions=genres,samples=modals) #make a table
```

### Reuters Corpus
```python
from nltk.corpus import reuters
reuters.fileids()
reuters.categories()
```
* Over 10,000 news documents, many overlap since they contain the same topic.
* Fetch via categories or fileids.

### Inaugural Address Corpus 

```python
from nltk.corpus import inaugural
inaugural.fileids()
# Use of Conditional Freq Dist again
cfd = nltk.ConditionalFreqDist((target,fileid[:4]) #[:4] extracts the years
	for fileid in inaugural.fileids() # years
	for w in inaugural.words(fileid) # get the target words
	for target in ['america','citizen']) # Specify the target

cfd.plot() #Plots two dashed lines
```
### Annotated Text corpora
* A list of corpora with linguistic tags can be found [here](http://www.nltk.org/book/ch02#tab-corpora "corpora")

### Corpora in other languages 
* [Chapter 3.3](http://www.nltk.org/book/ch03.html#sec-unicode) specified the use of other corpora. (**Encoding matters!**)
* *udhr*: Universal Declaration of Human Rights in over 300 languages

```python
fileids = udhr.fileids()
raw_text = udhr.raw('some-language')
nltk.FreqDist(raw_text).plot()
```

### Text corpus structure
* Isolated: just a collection of texts.
* Categorized: Correspond to genre, source, author, language.
* Overlappint: in case of topical categories, text can be relevant to many topics(e.g. Reuters)
* Temporal: News, Addresses.
* **Basic corpus Functionalities:**
	1. fileids([categories])
	2. categories([fileids])
	3. raw(fileids|categories=[...])
	4. words(fileids|categories=[...])
	5. sents(fileids|categories=[...])
	6. abspath(fileid) (Location of the given file on disk)
	7. endoding(fileid)
	8. open(fileid)
	9. readme()
	10. root

### Loading your own corpus: Use PlainTextCorpusReader or BracktParseCorpusReader
```python
from nltk.corpus import PlaintextCorpusReader 
corpus_root = '/path/to/folder/with/texts'
wordLists = PlaintextCorpusReader(corpus_root,'.*') #pass in list of fields or a regex pattern that matches all fileids as the second arg
wordLists.fileids()
wordlists.words(fileid)


```
[Source code to BracketParseCorpusReader](http://www.nltk.org/book/ch02#corpus-root-treebank)

## Conditional Frequency Distributions





